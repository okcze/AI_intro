{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unavailable-indonesia",
   "metadata": {},
   "source": [
    "# PD 6\n",
    "## Piotr Fic\n",
    "### Import bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hawaiian-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-batman",
   "metadata": {},
   "source": [
    "## Funkcje pomocnicze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-christopher",
   "metadata": {},
   "source": [
    "### Aktywacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fourth-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    exp = np.exp(y-np.max(y))\n",
    "    return exp/exp.sum(axis = 0, keepdims=True)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_der(x):\n",
    "    return 1\n",
    "\n",
    "def tanh(x):\n",
    "    exp = np.exp(x)\n",
    "    exp_ = np.exp(-x)\n",
    "    return (exp - exp_)/(exp + exp_)\n",
    "\n",
    "def tanh_der(x):\n",
    "    return (1-tanh(x)**2)\n",
    "\n",
    "def ReLu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def ReLu_der(x):\n",
    "    return np.where(x>0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-thailand",
   "metadata": {},
   "source": [
    "### Miary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "competitive-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predicted, real):\n",
    "    return np.mean((predicted - real)**2)\n",
    "\n",
    "def MAE(predicted, real):\n",
    "    return np.mean(np.abs(predicted - real))\n",
    "\n",
    "def acc(x, y):\n",
    "    return (x==y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-growing",
   "metadata": {},
   "source": [
    "### Wykresy i dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "smart-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    y = y.astype(int)\n",
    "    n_values = np.max(y) + 1\n",
    "    return np.eye(n_values)[y.T][0]\n",
    "\n",
    "def prepear_classif(df_train_p, df_test_p):\n",
    "    df_train = pd.read_csv(df_train_p)\n",
    "    df_test = pd.read_csv(df_test_p)\n",
    "    #Separacja wektorów cech i odpowiedzi\n",
    "    x_train, x_test = df_train.iloc[:, 0:2], df_test.iloc[:, 0:2]\n",
    "    x_train, x_test = np.array(x_train), np.array(x_test)\n",
    "    y_train, y_test = np.array(df_train.iloc[:, 2:3]), np.array(df_test.iloc[:, 2:3])\n",
    "    #Normalizacja\n",
    "    x_train, x_test = normalize(x_train), normalize(x_test)\n",
    "    return x_train, one_hot(y_train), x_test, y_test\n",
    "\n",
    "def prepear_data(df_train_p, df_test_p):\n",
    "    df_train = pd.read_csv(df_train_p)\n",
    "    df_test = pd.read_csv(df_test_p)\n",
    "    #Separacja wektorów cech i odpowiedzi\n",
    "    x_train, x_test = df_train.iloc[:, 1], df_test.iloc[:, 1]\n",
    "    x_train, x_test = np.array(x_train), np.array(x_test)\n",
    "    y_train, y_test = np.array(df_train.iloc[:, 2:3]), np.array(df_test.iloc[:, 2:3])\n",
    "    #Normalizacja\n",
    "    x_train, x_test = normalize(x_train), normalize(x_test)\n",
    "    y_train, y_test = normalize(y_train), normalize(y_test)\n",
    "    return x_train.reshape(-1, 1), y_train.reshape(-1, 1), x_test.reshape(-1, 1), y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "genuine-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_real(predicted, real):\n",
    "    plt.scatter(real, predicted, c='blue')\n",
    "    p1 = max(max(predicted), max(real))\n",
    "    p2 = min(min(predicted), min(real))\n",
    "    plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "    plt.xlabel('True values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title('Predictions visualization on test set')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-laser",
   "metadata": {},
   "source": [
    "## Implementacja MLP\n",
    " - dodanie uczenia z karą L2\n",
    " - implementacja uczenia ze zbiorem walidacyjnym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incomplete-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "        \n",
    "    def __init__(self, \n",
    "                 #Liczba neuronów w poprzedzającej i kolejnej warstwie\n",
    "                 input_size: int, \n",
    "                 output_size: int,\n",
    "                 #Domyślna funkcja aktywacyjna i jej pochodna                 \n",
    "                 activation_fun = sigmoid, \n",
    "                 activation_fun_der = sigmoid_der,\n",
    "                 ):\n",
    "        \n",
    "        #Wagi dla wszystkich neuronów - inicjalizacja Xavier\n",
    "        self.biases = np.random.uniform(\n",
    "            -math.sqrt(6)/ math.sqrt(input_size + output_size),\n",
    "            math.sqrt(6)/ math.sqrt(input_size + output_size),\n",
    "                    size=(output_size, 1)\n",
    "                )\n",
    "        \n",
    "        self.weights = np.random.uniform(\n",
    "            -math.sqrt(6)/ math.sqrt(input_size + output_size),\n",
    "            math.sqrt(6)/ math.sqrt(input_size + output_size),\n",
    "                    size=(output_size, input_size)\n",
    "                )\n",
    "            \n",
    "        #Funkcja aktywacji i jej pochodna\n",
    "        self.activation_fun = activation_fun\n",
    "        self.activation_fun_der = activation_fun_der\n",
    "    \n",
    "    def predict(self, input):\n",
    "        #Przekształca input z poprzedniej warstwy przez wagi i funkcję aktywacji \n",
    "        #Zwraca output do przekazania kolejnej warstwie\n",
    "        return self.activation_fun(self.weights@input + self.biases)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #Przekształca input z poprzedniej warstwy jedynie przez wagi\n",
    "        return self.weights@input + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "extreme-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    def __init__(self, layers: list, classif: bool):\n",
    "        #classif\n",
    "        self.classif = classif\n",
    "        #Warstwy\n",
    "        self.layers = layers\n",
    "        self.momentum_w = [np.zeros(l.weights.shape) for l in layers]\n",
    "        self.momentum_b = [np.zeros(l.biases.shape) for l in layers]\n",
    "        self.g_w = [np.zeros(l.weights.shape) for l in layers]\n",
    "        self.g_b = [np.zeros(l.biases.shape) for l in layers]\n",
    "        \n",
    "        if classif:\n",
    "            self.layers[-1].activation_fun = softmax\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Oblicza output na podstawie danych i parametrów warstw\"\"\"\n",
    "        output = X.T\n",
    "        for layer in self.layers:\n",
    "            output = layer.predict(output)\n",
    "        return output.T\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Pełna metoda feedforward\n",
    "        return: sumy, aktywacje\"\"\"\n",
    "        sums = []\n",
    "        activations = [X]\n",
    "        activations_layer = X\n",
    "        for layer in self.layers:\n",
    "            sums_layer = layer.forward(activations_layer)\n",
    "            sums.append(sums_layer)\n",
    "            \n",
    "            activations_layer = layer.activation_fun(sums_layer)\n",
    "            activations.append(activations_layer)\n",
    "            \n",
    "        return sums, activations\n",
    "    \n",
    "    def L2_cost(self):\n",
    "        \"\"\"Funkcja pomocnicza, licząca części funkcji kosztu\"\"\"\n",
    "        suma = 0\n",
    "        for i in range(len(self.layers)):\n",
    "            suma += np.sum(np.square(self.layers[i].weights))\n",
    "        return suma\n",
    "    \n",
    "    def backprop(self, X, Y, l2):\n",
    "        \"\"\"Propagacja wsteczna błędu\n",
    "        return: gradienty MSE\"\"\"\n",
    "        #Batch size\n",
    "        batch_size = Y.shape[1]\n",
    "        \n",
    "        #Wyliczenie feedforward obecnymi parametrami\n",
    "        sums, activations = self.forward(X)\n",
    "        \n",
    "        #Macierze na poprawki parametrów\n",
    "        delta_biases = []\n",
    "        delta_weights = []\n",
    "        \n",
    "        n_layers = len(self.layers)\n",
    "        err = [None]*n_layers\n",
    "        \n",
    "        #Wyliczenie err ostatniej warstwy\n",
    "        if self.classif:\n",
    "            err[-1] = Y - activations[-1]\n",
    "        else:\n",
    "            err[-1] = (Y - activations[-1])*self.layers[-1].activation_fun_der(sums[-1])\n",
    "        \n",
    "        #Regularyzacja L2\n",
    "        err[-1] = err[-1] + l2*self.L2_cost()/(2*batch_size)\n",
    "        \n",
    "        #Propagacja wsteczna\n",
    "        for i in reversed(range(len(err) -1)):\n",
    "            act_f_der = self.layers[i].activation_fun_der(sums[i])\n",
    "            err[i] = (self.layers[i+1].weights.transpose()@err[i+1]) * act_f_der \n",
    "                        \n",
    "        delta_biases = [e@np.ones((batch_size, 1))/float(batch_size) \n",
    "                        for e in err]\n",
    "        delta_weights = [np.dot(e, activations[i].transpose())/float(batch_size) + l2*self.layers[i].weights/batch_size\n",
    "                        for i, e in enumerate(err)]\n",
    "        \n",
    "        return delta_biases, delta_weights\n",
    "    \n",
    "    def train(self, X, Y, \n",
    "              batch_size = 1, \n",
    "              etha = 0.001, \n",
    "              tol = 10**(-6), \n",
    "              n_iter = 500,\n",
    "              #Regularyzacja L2\n",
    "              l2 = 0,\n",
    "              #Lambda dla momentum\n",
    "              l_m = 0,\n",
    "              #Beta dla RMSProp\n",
    "              beta = 1,\n",
    "              verbose = False):\n",
    "        \n",
    "        it = 0\n",
    "        while it<n_iter:\n",
    "            #Losowość przed podziałem zbioru, ziarno zapewnia identyczną permutację\n",
    "            #w zmiennych objaśnających i zmiennej celu\n",
    "            idx = random.permutation(X.shape[0])\n",
    "            X_train = np.copy(X[idx])\n",
    "            Y_train = np.copy(Y[idx])\n",
    "            \n",
    "            self.batch_gd(X_train, Y_train, batch_size, etha, l_m, beta, l2)\n",
    "            \n",
    "            #Wizualizacja procesu uczenia\n",
    "            if(verbose):\n",
    "                print(\"Epoche \" + str(it) + \" finished\")\n",
    "                for i in range(len(self.layers)):\n",
    "                    print(\"Warstwa \" + str(i) + \" wagi:\")\n",
    "                    print(self.layers[i].weights)\n",
    "                    print(\"Warstwa \" + str(i) + \" bias-y:\")\n",
    "                    print(self.layers[i].biases)\n",
    "                print(\"\\n\")\n",
    "            \n",
    "            it += 1\n",
    "            \n",
    "        return it\n",
    "    \n",
    "    def train_valid(self, X, Y, \n",
    "              batch_size = 1, \n",
    "              etha = 0.001,\n",
    "              #Uczenie ze zbiorem walidacyjnym\n",
    "              score_fun = MSE,      \n",
    "              tol = 10**(-6), \n",
    "              n_iter = 500,\n",
    "              #Regularyzacja L2\n",
    "              l2 = 0,\n",
    "              #Lambda dla momentum\n",
    "              l_m = 0,\n",
    "              #Beta dla RMSProp\n",
    "              beta = 1,\n",
    "              verbose = False):\n",
    "        \"\"\"Trening sieci z kontrolą błędu na zbiorze walidacyjnym\"\"\"\n",
    "        idx = random.permutation(X.shape[0])\n",
    "        X = np.copy(X[idx])\n",
    "        Y = np.copy(Y[idx])\n",
    "            \n",
    "        sp = int(0.8*len(X))\n",
    "        X_val = X[sp:]\n",
    "        Y_val = Y[sp:]\n",
    "        X = X[:sp]\n",
    "        Y = Y[:sp]\n",
    "        \n",
    "        it = 0\n",
    "        old_score = 100\n",
    "        new_score = 10\n",
    "        while (old_score-new_score)>tol:\n",
    "            old_score = new_score\n",
    "            \n",
    "            #Losowość przed podziałem zbioru, ziarno zapewnia identyczną permutację\n",
    "            #w zmiennych objaśnających i zmiennej celu\n",
    "            idx = random.permutation(X.shape[0])\n",
    "            X_train = np.copy(X[idx])\n",
    "            Y_train = np.copy(Y[idx])\n",
    "            \n",
    "            self.batch_gd(X_train, Y_train, batch_size, etha, l_m, beta, l2)\n",
    "            \n",
    "            pred = self.predict(X_val)\n",
    "            new_score = score_fun(pred, Y_val)\n",
    "            \n",
    "            #Wizualizacja procesu uczenia\n",
    "            if(verbose):\n",
    "                print(\"Epoche \" + str(it) + \" finished\")\n",
    "                for i in range(len(self.layers)):\n",
    "                    print(\"Warstwa \" + str(i) + \" wagi:\")\n",
    "                    print(self.layers[i].weights)\n",
    "                    print(\"Warstwa \" + str(i) + \" bias-y:\")\n",
    "                    print(self.layers[i].biases)\n",
    "                print(\"\\n\")\n",
    "            \n",
    "            it += 1\n",
    "        \n",
    "        return it\n",
    "    \n",
    "    def gd(self, X, Y, etha, l_m, beta, l2):\n",
    "        \"\"\"Trening sieci podstawową metodą Gradient Descent\"\"\"\n",
    "        b, w = self.backprop(X, Y, l2)\n",
    "        for i in range(len(self.layers)):\n",
    "            #Aktualizacja momentum\n",
    "            self.momentum_w[i] = w[i]+l_m*self.momentum_w[i]\n",
    "            self.momentum_b[i] = b[i]+l_m*self.momentum_b[i]\n",
    "            \n",
    "            #Aktualizacja RMSProp\n",
    "            self.g_w[i] = (1-beta)*w[i]*w[i]+beta*self.g_w[i]\n",
    "            self.g_b[i] = (1-beta)*b[i]*b[i]+beta*self.g_b[i]\n",
    "        \n",
    "            l = self.layers[i]\n",
    "            if(beta):\n",
    "                l.biases = l.biases + etha*self.momentum_b[i] + (etha*b[i])/np.sqrt(0.00001+self.g_b[i]) \n",
    "                l.weights = l.weights + etha*self.momentum_w[i] + (etha*w[i])/np.sqrt(0.00001+self.g_w[i])\n",
    "            else:\n",
    "                l.biases = l.biases + etha*self.momentum_b[i]\n",
    "                l.weights = l.weights + etha*self.momentum_w[i]\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def batch_gd(self, X, Y, batch_size, etha, l_m, beta, l2):\n",
    "        \"\"\"Trening sieci metodą Mini-batch Gradient Descent\"\"\"\n",
    "        #Metoda train uprzednio dokonuje permutacji zbioru\n",
    "        #Wywołanie metody gradient descent na kolejnych batch-ach\n",
    "        i=0\n",
    "        while(i<len(Y)):\n",
    "            x = X[i:i+batch_size]\n",
    "            y = Y[i:i+batch_size]\n",
    "            i += batch_size\n",
    "            self.gd(x.T, y.T, etha, l_m, beta, l2)\n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-saskatchewan",
   "metadata": {},
   "source": [
    "# Test działania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-secretariat",
   "metadata": {},
   "source": [
    "### architektura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "greater-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-5-5-1\n",
    "def arch_2(f, f_d, classif, inp=1, outp=1):\n",
    "    l1 = Layer(inp, 5, activation_fun=f, activation_fun_der=f_d)\n",
    "    l2 = Layer(5, 5, activation_fun=f, activation_fun_der=f_d)\n",
    "    l3 = Layer(5, outp, activation_fun=linear, activation_fun_der=linear_der)\n",
    "    return Network([l1, l2, l3], classif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-consolidation",
   "metadata": {},
   "source": [
    "## Regresja\n",
    "### df: multimodal-sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "opposite-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = prepear_data(\"./mio1/regression/multimodal-sparse-training.csv\",\n",
    "                                               \"./mio1/regression/multimodal-sparse-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "formal-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 10\n",
    "val = [0]*ep\n",
    "raw = [0]*ep\n",
    "reg = [0]*ep\n",
    "\n",
    "for i in range(ep):\n",
    "    n_val = arch_2(tanh, tanh_der, False)\n",
    "    n_raw = arch_2(tanh, tanh_der, False)\n",
    "    n_reg = arch_2(tanh, tanh_der, False)\n",
    "\n",
    "    #with valid\n",
    "    n_val.train_valid(x_train, y_train, batch_size=40, etha = 0.0001, tol=10**(-3))\n",
    "    res = n_val.predict(x_test)\n",
    "    val[i] = MSE(y_test, res)\n",
    "    #no valid\n",
    "    n_raw.train(x_train, y_train, batch_size=40, etha = 0.0001, n_iter=10)\n",
    "    res = n_raw.predict(x_test)\n",
    "    raw[i] = MSE(y_test, res)\n",
    "    #L2\n",
    "    n_reg.train(x_train, y_train, batch_size=40, etha = 0.0001, n_iter=10, l2 = 0.8)\n",
    "    res = n_reg.predict(x_test)\n",
    "    reg[i] = MSE(y_test, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "dominant-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame.from_records(np.array([raw, val, reg]).T, columns=[\"no_reg\", \"valid_df\", \"L2_pen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-found",
   "metadata": {},
   "source": [
    "#### średnie MSE na zbiorze testowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "unavailable-celebration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_reg</th>\n",
       "      <th>valid_df</th>\n",
       "      <th>L2_pen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.156746</td>\n",
       "      <td>0.090242</td>\n",
       "      <td>0.129759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.127329</td>\n",
       "      <td>0.051691</td>\n",
       "      <td>0.059867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.064654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.064989</td>\n",
       "      <td>0.061945</td>\n",
       "      <td>0.082422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.135586</td>\n",
       "      <td>0.078096</td>\n",
       "      <td>0.112867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.199672</td>\n",
       "      <td>0.103328</td>\n",
       "      <td>0.174530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.459853</td>\n",
       "      <td>0.190587</td>\n",
       "      <td>0.238202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          no_reg   valid_df     L2_pen\n",
       "count  10.000000  10.000000  10.000000\n",
       "mean    0.156746   0.090242   0.129759\n",
       "std     0.127329   0.051691   0.059867\n",
       "min     0.028333   0.017755   0.064654\n",
       "25%     0.064989   0.061945   0.082422\n",
       "50%     0.135586   0.078096   0.112867\n",
       "75%     0.199672   0.103328   0.174530\n",
       "max     0.459853   0.190587   0.238202"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-psychiatry",
   "metadata": {},
   "source": [
    "## Klasyfikacja\n",
    "### df: rings5-sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "australian-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = prepear_classif(\"./mio1/classification/rings5-sparse-training.csv\",\n",
    "                                               \"./mio1/classification/rings5-sparse-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "formal-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 20\n",
    "val = [0]*ep\n",
    "raw = [0]*ep\n",
    "reg = [0]*ep\n",
    "\n",
    "for i in range(ep):\n",
    "    n_val = arch_2(tanh, tanh_der, inp=2, outp=5, classif=True)\n",
    "    n_raw = arch_2(tanh, tanh_der, inp=2, outp=5, classif=True)\n",
    "    n_reg = arch_2(tanh, tanh_der, inp=2, outp=5, classif=True)\n",
    "\n",
    "    #with valid\n",
    "    n_val.train_valid(x_train, y_train, batch_size=200, etha = 0.001, tol=10**(-5))\n",
    "    res = n_val.predict(x_test)\n",
    "    pred = np.argmax(res, axis=1)\n",
    "    val[i] = acc(y_test, pred)\n",
    "    #no valid\n",
    "    n_raw.train(x_train, y_train, batch_size=200, etha = 0.001, n_iter=20)\n",
    "    res = n_raw.predict(x_test)\n",
    "    pred = np.argmax(res, axis=1)\n",
    "    raw[i] = acc(y_test, pred)\n",
    "    #L2\n",
    "    n_reg.train(x_train, y_train, batch_size=200, etha = 0.001, n_iter=20, l2 = 0.3)\n",
    "    res = n_reg.predict(x_test)\n",
    "    pred = np.argmax(res, axis=1)\n",
    "    reg[i] = acc(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "continued-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame.from_records(np.array([raw, val, reg]).T, columns=[\"no_reg\", \"valid_df\", \"L2_pen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-publication",
   "metadata": {},
   "source": [
    "#### średnie ACC na zbiorze testowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "rational-commissioner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_reg</th>\n",
       "      <th>valid_df</th>\n",
       "      <th>L2_pen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.178839</td>\n",
       "      <td>0.198047</td>\n",
       "      <td>0.191973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.040811</td>\n",
       "      <td>0.048099</td>\n",
       "      <td>0.039097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.082607</td>\n",
       "      <td>0.077964</td>\n",
       "      <td>0.124287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.149131</td>\n",
       "      <td>0.178914</td>\n",
       "      <td>0.168778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.192933</td>\n",
       "      <td>0.204859</td>\n",
       "      <td>0.184910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.200474</td>\n",
       "      <td>0.237113</td>\n",
       "      <td>0.206075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.260362</td>\n",
       "      <td>0.261337</td>\n",
       "      <td>0.289853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          no_reg   valid_df     L2_pen\n",
       "count  20.000000  20.000000  20.000000\n",
       "mean    0.178839   0.198047   0.191973\n",
       "std     0.040811   0.048099   0.039097\n",
       "min     0.082607   0.077964   0.124287\n",
       "25%     0.149131   0.178914   0.168778\n",
       "50%     0.192933   0.204859   0.184910\n",
       "75%     0.200474   0.237113   0.206075\n",
       "max     0.260362   0.261337   0.289853"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-absorption",
   "metadata": {},
   "source": [
    "### df: rings3-balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "opposite-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = prepear_classif(\"./mio1/classification/rings3-balance-training.csv\",\n",
    "                                               \"./mio1/classification/rings3-balance-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "domestic-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./mio1/classification/rings3-balance-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "formal-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 5\n",
    "val = [0]*ep\n",
    "raw = [0]*ep\n",
    "reg = [0]*ep\n",
    "\n",
    "for i in range(ep):\n",
    "    n_val = arch_2(tanh, tanh_der, inp=2, outp=3, classif=True)\n",
    "    n_raw = arch_2(tanh, tanh_der, inp=2, outp=3, classif=True)\n",
    "    n_reg = arch_2(tanh, tanh_der, inp=2, outp=3, classif=True)\n",
    "\n",
    "    #with valid\n",
    "    n_val.train_valid(x_train, y_train, batch_size=2060, etha = 0.001, tol=10**(-7))\n",
    "    res = n_val.predict(x_test)\n",
    "    pred = np.argmax(res, axis=1)\n",
    "    val[i] = acc(y_test, pred)\n",
    "    #no valid\n",
    "    n_raw.train(x_train, y_train, batch_size=2060, etha = 0.001, n_iter=500)\n",
    "    res = n_raw.predict(x_test)\n",
    "    pred = np.argmax(res, axis=1)\n",
    "    raw[i] = acc(y_test, pred)\n",
    "    #L2\n",
    "    n_reg.train(x_train, y_train, batch_size=2060, etha = 0.001, n_iter=500, l2 = 0.6)\n",
    "    res = n_reg.predict(x_test)\n",
    "    pred = np.argmax(res, axis=1)\n",
    "    reg[i] = acc(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "reasonable-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame.from_records(np.array([raw, val, reg]).T, columns=[\"no_reg\", \"valid_df\", \"L2_pen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-separation",
   "metadata": {},
   "source": [
    "#### średnie ACC na zbiorze testowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "unknown-reader",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_reg</th>\n",
       "      <th>valid_df</th>\n",
       "      <th>L2_pen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.317618</td>\n",
       "      <td>0.310926</td>\n",
       "      <td>0.321991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.003446</td>\n",
       "      <td>0.007409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.315827</td>\n",
       "      <td>0.307533</td>\n",
       "      <td>0.312717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.316675</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.319031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.317335</td>\n",
       "      <td>0.310643</td>\n",
       "      <td>0.320634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.318466</td>\n",
       "      <td>0.312339</td>\n",
       "      <td>0.324875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.319785</td>\n",
       "      <td>0.316015</td>\n",
       "      <td>0.332697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         no_reg  valid_df    L2_pen\n",
       "count  5.000000  5.000000  5.000000\n",
       "mean   0.317618  0.310926  0.321991\n",
       "std    0.001549  0.003446  0.007409\n",
       "min    0.315827  0.307533  0.312717\n",
       "25%    0.316675  0.308098  0.319031\n",
       "50%    0.317335  0.310643  0.320634\n",
       "75%    0.318466  0.312339  0.324875\n",
       "max    0.319785  0.316015  0.332697"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-assembly",
   "metadata": {},
   "source": [
    "### df: xor3-balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "opposite-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = prepear_classif(\"./mio1/classification/xor3-balance-training.csv\",\n",
    "                                               \"./mio1/classification/xor3-balance-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "formal-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 5\n",
    "val = [0]*ep\n",
    "raw = [0]*ep\n",
    "reg = [0]*ep\n",
    "\n",
    "for i in range(ep):\n",
    "    n_val = arch_2(tanh, tanh_der, inp=2, outp=2, classif=True)\n",
    "    n_raw = arch_2(tanh, tanh_der, inp=2, outp=2, classif=True)\n",
    "    n_reg = arch_2(tanh, tanh_der, inp=2, outp=2, classif=True)\n",
    "\n",
    "    #with valid\n",
    "    n_val.train_valid(x_train, y_train, batch_size=40, beta=0.9, l_m=0.9, etha = 0.001, tol=10**(-5))\n",
    "    res = n_val.predict(x_test)\n",
    "    pred = np.argmax(res, axis=1)\n",
    "    val[i] = acc(y_test, pred)\n",
    "    #no valid\n",
    "    n_raw.train(x_train, y_train, batch_size=50, beta=0.9, l_m=0.9, etha = 0.001, n_iter=4000)\n",
    "    res = n_raw.predict(x_test)\n",
    "    pred = np.argmax(res, axis=1)\n",
    "    raw[i] = acc(y_test, pred)\n",
    "    #L2\n",
    "    n_reg.train(x_train, y_train, batch_size=50, beta=0.9, l_m=0.9, etha = 0.001, n_iter=4000, l2 = 0.0001)\n",
    "    res = n_reg.predict(x_test)\n",
    "    pred = np.argmax(res, axis=1)\n",
    "    reg[i] = acc(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "overhead-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame.from_records(np.array([raw, val, reg]).T, columns=[\"no_reg\", \"valid_df\", \"L2_pen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-cycle",
   "metadata": {},
   "source": [
    "#### średnie ACC na zbiorze testowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "front-arbor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_reg</th>\n",
       "      <th>valid_df</th>\n",
       "      <th>L2_pen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.529846</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.535884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.003703</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.524675</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.520606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.527956</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.528613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.529925</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.534650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.533075</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.546462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.533600</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>0.549087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         no_reg  valid_df    L2_pen\n",
       "count  5.000000    5.0000  5.000000\n",
       "mean   0.529846    0.5525  0.535884\n",
       "std    0.003703    0.0000  0.011980\n",
       "min    0.524675    0.5525  0.520606\n",
       "25%    0.527956    0.5525  0.528613\n",
       "50%    0.529925    0.5525  0.534650\n",
       "75%    0.533075    0.5525  0.546462\n",
       "max    0.533600    0.5525  0.549087"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-chest",
   "metadata": {},
   "source": [
    "## Wnioski\n",
    "Dla części zbiorów możemy zaobserwować poprawę wyników po regularyzacji na zbiorze testowym. Z drugiej strony osiągane miary są ogółem dość słabe w związku z utrudnioną strukturą zbiorów i małymi rozmiarami części treningowych względem testowych. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
