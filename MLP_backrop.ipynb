{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "attached-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "south-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Funkcja sigmoidalna\"\"\"\n",
    "    return 1/(1 + math.exp(-x))\n",
    "sigmoid = np.vectorize(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unexpected-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_der(x):\n",
    "    \"\"\"Pochodna funkcji sigmoidalnej\"\"\"\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "sigmoid_der = np.vectorize(sigmoid_der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "legitimate-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predicted, real):\n",
    "    return np.mean((predicted - real)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "efficient-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(predicted, real):\n",
    "    return np.mean(np.abs(predicted - real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "nuclear-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcje pmocnicze\n",
    "def normalize(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "def prepear_data(df_train, df_test):\n",
    "    #Separacja wektorów cech i odpowiedzi\n",
    "    x_train, x_test = df_train.iloc[:, 1], df_test.iloc[:, 1]\n",
    "    x_train, x_test = np.array(x_train), np.array(x_test)\n",
    "    #Implementacja wymaga \"pionowego\" wektora cech\n",
    "    x_train.shape = (len(x_train), 1)\n",
    "    x_test.shape = (len(x_test), 1)\n",
    "    y_train, y_test = np.array(df_train.iloc[:, 2:3]), np.array(df_test.iloc[:, 2:3])\n",
    "    #Normalizacja\n",
    "    x_train, x_test = normalize(x_train), normalize(x_test)\n",
    "    y_train, y_test = normalize(y_train), normalize(y_test)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "alert-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "###ZMIANY\n",
    "# - wektoryzacja funkcji pomocniczych przy definiowaniu\n",
    "# - oddzielne metody Network predict i forward\n",
    "# - oddzielne metody predict i forward w Layer\n",
    "#   (z lub bez obłożenia funkcją aktywacji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "suitable-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "        \n",
    "    def __init__(self, \n",
    "                 #Liczba neuronów w poprzedzającej i kolejnej warstwie\n",
    "                 input_size: int, \n",
    "                 output_size: int,\n",
    "                 #Domyślna funkcja aktywacyjna i jej pochodna                 \n",
    "                 activation_fun = sigmoid, \n",
    "                 activation_fun_der = sigmoid_der,\n",
    "                 #Możliwe do ustawienia wagi i bias-y\n",
    "                 weights = None, \n",
    "                 biases = None):\n",
    "        \n",
    "        #Domyślne losowo wygenerowane wagi i bias-y jeśli nie zostały podane\n",
    "        d_weights = random.uniform(-1, 1, size = (input_size, output_size))\n",
    "        d_biases = random.uniform(-1, 1, size = (1, output_size))\n",
    "        \n",
    "        #Wagi dla wszystkich neuronów z warstwy \n",
    "        self.weights = weights if weights is not None else d_weights\n",
    "        \n",
    "        #Stałe \"b\"\n",
    "        self.biases = biases if biases is not None else d_biases\n",
    "        \n",
    "        #Funkcja aktywacji i jej pochodna\n",
    "        self.activation_fun = activation_fun\n",
    "        self.activation_fun_der = activation_fun_der\n",
    "    \n",
    "    def predict(self, input):\n",
    "        #Przekształca input z poprzedniej warstwy przez wagi i funkcję aktywacji \n",
    "        #Zwraca output do przekazania kolejnej warstwie\n",
    "        return self.activation_fun(input @ self.weights + self.biases)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #Przekształca input z poprzedniej warstwy jedynie przez wagi\n",
    "        return input @ self.weights + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "developing-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    def __init__(self, layers: list):\n",
    "        \n",
    "        #Warstwy\n",
    "        self.layers = layers\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Oblicza output na podstawie danych i parametrów warstw\"\"\"\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.predict(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Pełna metoda feedforward\n",
    "        return: sumy, aktywacje\"\"\"\n",
    "        sums = []\n",
    "        activations = [X]\n",
    "        activations_layer = X\n",
    "        for layer in self.layers:\n",
    "            sums_layer = layer.forward(activations_layer)\n",
    "            sums.append(sums_layer)\n",
    "            \n",
    "            activations_layer = layer.activation_fun(sums_layer)\n",
    "            activations.append(activations_layer)\n",
    "            \n",
    "        return sums, activations\n",
    "    \n",
    "    def backprop(self, X, Y):\n",
    "        \"\"\"Propagacja wsteczna błędu\n",
    "        return: gradienty MSE\"\"\"\n",
    "        \n",
    "        #Wyliczenie feedforward obecnymi parametrami\n",
    "        sums, activations = self.forward(X)\n",
    "        \n",
    "        #Macierze na poprawki parametrów\n",
    "        delta_biases = [np.zeros(l.biases.shape) for l in self.layers]\n",
    "        delta_weights = [np.zeros(l.weights.shape) for l in self.layers]\n",
    "        \n",
    "        #Wyliczenie err ostatniej warstwy\n",
    "        err = (activations[-1] - Y)*self.layers[-1].activation_fun_der(sums[-1])\n",
    "        \n",
    "        #Wyliczenie gradientu MSE po parametrach ostatniej warstwy\n",
    "        delta_biases[-1] = err\n",
    "        delta_weights[-1] = activations[-2].transpose()@err\n",
    "        \n",
    "        #Propagacja wsteczna\n",
    "        n_layers = len(self.layers)\n",
    "        \n",
    "        for i in range(n_layers-2, -1, -1):\n",
    "            act_f_der = self.layers[i].activation_fun_der(sums[i])\n",
    "            err = (err@self.layers[i+1].weights.transpose()) * act_f_der \n",
    "            delta_biases[i] = err\n",
    "            delta_weights[i] = np.dot(activations[i-1].transpose(), err)\n",
    "        \n",
    "        return delta_biases, delta_weights\n",
    "    \n",
    "    def train(self, X, Y, method: str, etha = 0.001, tol = 0.001):\n",
    "        \"\"\"Wytrenowuje sieć wybraną metodą minimalizując MSE \n",
    "        na zbiorze walidacyjnym\"\"\"\n",
    "        \n",
    "        etha = etha/len(X)\n",
    "        \n",
    "        if(method == \"gd\" or method is None):\n",
    "            algorithm = self.gd\n",
    "        elif(method == \"batch_gd\"):\n",
    "            print(\"TODO\")\n",
    "        \n",
    "        #Losowość przed podziałem zbioru\n",
    "        random.RandomState(42).shuffle(X)\n",
    "        random.RandomState(42).shuffle(Y)\n",
    "        \n",
    "        #Podział zbioru na część walidacyjną i treningową\n",
    "        X_split = np.split(X, [int(.8 * len(X))])\n",
    "        X_train, X_val = X_split[0], X_split[1]\n",
    "        \n",
    "        Y_split = np.split(Y, [int(.8 * len(Y))])\n",
    "        Y_train, Y_val = Y_split[0], Y_split[1]\n",
    "        \n",
    "        #Trening z walidacją\n",
    "        current_mse = np.inf\n",
    "        new_mse = MSE(self.predict(X_val), Y_val)\n",
    "        it = 1\n",
    "        while True:\n",
    "            #Wywołanie konkretnego algorytmu\n",
    "            algorithm(X_train, Y_train, it, etha)\n",
    "            new_mse = MSE(self.predict(X_val), Y_val)\n",
    "            \n",
    "            it += 1\n",
    "            print(\"Current MSE on validation set:\")\n",
    "            print(new_mse)\n",
    "            \n",
    "            if(current_mse - new_mse > tol):\n",
    "                current_mse = new_mse\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        print(\"Final MSE on validation set:\")\n",
    "        print(current_mse)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def gd(self, X, Y, it, etha):\n",
    "        \"\"\"Trening sieci podstawową metodą Gradient Descent\"\"\"\n",
    "        delta_biases = [np.zeros(l.biases.shape) for l in self.layers]\n",
    "        delta_weights = [np.zeros(l.weights.shape) for l in self.layers]\n",
    "        \n",
    "        for x, y in zip(X, Y):\n",
    "            b, w = self.backprop(x, y)\n",
    "            delta_biases = [cb-etha*nb for cb, nb in zip(delta_biases, b)]\n",
    "            delta_weights = [cw-etha*nw for cw, nw in zip(delta_weights, w)]\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            l = self.layers[i]\n",
    "            l.biases = l.biases + delta_biases[i]\n",
    "            l.weights = l.weights + delta_weights[i]\n",
    "        \n",
    "        print(\"Epoche \" + str(it) + \" finished\")\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "satisfactory-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_train = pd.read_csv(\"./mio1/regression/steps-small-training.csv\")\n",
    "df1_test = pd.read_csv(\"./mio1/regression/steps-small-test.csv\")\n",
    "x_train, y_train, x_test, y_test = prepear_data(df1_train, df1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fantastic-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utworzenie warst z dobranymi parametrami\n",
    "l1 = Layer(input_size=1, output_size=5, \n",
    "           weights=np.array([[0.75, -0.8, 0.4, -0.7, 0.4]]), \n",
    "           biases=np.array([[-1, -0.9, -0.9, -1, 0.2]]))\n",
    "          \n",
    "l2 = Layer(input_size=5, output_size=1, \n",
    "           activation_fun= lambda x: x, \n",
    "           activation_fun_der= lambda x: 1,\n",
    "           weights = np.array([[40], [40], [35], [35], [0]]),\n",
    "           biases = np.array([[-41.25]]))\n",
    "          \n",
    "\n",
    "#Konstrukcja obiektu sieci i predykcja na danych treningowych i testowych\n",
    "mlp = Network([l1, l2])\n",
    "#pred_train = mlp.forward(x_train)\n",
    "#pred_test = mlp.forward(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fancy-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utworzenie warst z dobranymi parametrami\n",
    "l1 = Layer(input_size=1, output_size=5)\n",
    "l2 = Layer(input_size=5, output_size=1, \n",
    "           activation_fun= lambda x: x, \n",
    "           activation_fun_der= lambda x: 1)          \n",
    "\n",
    "#Konstrukcja obiektu sieci i predykcja na danych treningowych i testowych\n",
    "mlp = Network([l1, l2])\n",
    "#pred_train = mlp.forward(x_train)\n",
    "#pred_test = mlp.forward(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "aware-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Architektura: 1 warstwa (10 neuronów) losowe parametry\n",
    "l1 = Layer(1, 10)\n",
    "l2 = Layer(10, 1, activation_fun= lambda x: x)\n",
    "\n",
    "mlp = Network([l1, l2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "prostate-reaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoche 1 finished\n",
      "Current MSE on validation set:\n",
      "0.14387624474472607\n",
      "Epoche 2 finished\n",
      "Current MSE on validation set:\n",
      "0.1438769811235842\n",
      "Final MSE on validation set:\n",
      "0.14387624474472607\n"
     ]
    }
   ],
   "source": [
    "mlp.train(x_train, y_train, \"gd\", etha=10**(-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "contemporary-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = mlp.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "optical-rental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33060205580705165\n"
     ]
    }
   ],
   "source": [
    "print(MAE(pred_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
